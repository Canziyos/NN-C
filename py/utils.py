import math

def MSE(y_true, y_pred):
    if len(y_true) == 0 or len(y_true) != len(y_pred):
        raise ValueError("MSE: y_true and y_pred must be non-empty and equal length.")
    n = len(y_true)
    return sum((t - p) ** 2 for t, p in zip(y_true, y_pred)) / n

# I know how wrong I was.
def output_layer_delta(y_true, y_pred, z_values, activation_prime):
    n = len(y_true)
    return [(2/n)*(yp - yt) * activation_prime(z)
            for yp, yt, z in zip(y_pred, y_true, z_values)]


def output_gradient(delta, prev_activations):
    # For each neuron j in the output layer:
    weight_grads = [[delta[j] * a for a in prev_activations]
                    for j in range(len(delta))]
    bias_grads = delta[:]  # just copy.
    return weight_grads, bias_grads

# How wrong I was depends on how much I influenced
# the next layerâ€™s mistakes, scaled by my own sensitivity.
def hidden_layer_delta(z_values, activation_prime, next_layer_weights, next_layer_delta):
    deltas = []
    for i, z in enumerate(z_values):
        # sum over next-layer neurons j: w_ij * delta_j.
        downstream = sum(next_layer_weights[j][i] * next_layer_delta[j] 
                         for j in range(len(next_layer_delta)))
        deltas.append(activation_prime(z) * downstream)
    return deltas


def cross_entropy(y_true, y_pred, eps=1e-15):
    """
    Cross-entropy loss for multi-class classification.
    y_true: one list of true labels (e.g., [0,1,0])
    y_pred: list of predicted probabilities (softmax output)
    eps: to avoid log(0)
    """
    if len(y_true) != len(y_pred):
        raise ValueError("cross_entropy: y_true and y_pred must be the same length")

    # Clip predictions to avoid log(0)
    y_pred = [min(max(p, eps), 1 - eps) for p in y_pred]

    return -sum(t * math.log(p) for t, p in zip(y_true, y_pred))
